{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4781b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mahotas\n",
      "  Downloading mahotas-1.4.18-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\prave\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mahotas) (1.26.4)\n",
      "Downloading mahotas-1.4.18-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mahotas\n",
      "Successfully installed mahotas-1.4.18\n"
     ]
    }
   ],
   "source": [
    "!pip install mahotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d91beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Image directory: C:\\Users\\prave\\Final Year project\\data\n",
      "üîç Collecting descriptors...\n",
      "‚úÖ Total descriptors collected: 493256\n",
      "üìä Clustering descriptors to form visual vocabulary...\n",
      "‚úÖ Dictionary shape: (600, 128)\n",
      "üîé Feature Extraction for all images...\n",
      "‚úÖ Feature extraction complete. Data saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mahotas\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Set the image directory path manually (change this as needed)\n",
    "IMDIR = r\"C:\\Users\\prave\\Final Year project\\data\"\n",
    "\n",
    "# Output file paths\n",
    "BOVW = \"model/bovw_codebook_600.pickle\"\n",
    "DICT_SIZE = 600\n",
    "DATA = 'model/data_600.npy'\n",
    "LABEL = 'model/label_600.npy'\n",
    "\n",
    "# Feature extractors\n",
    "def fd_hu_moments(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.HuMoments(cv2.moments(gray)).flatten()\n",
    "\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return mahotas.features.haralick(gray).mean(axis=0)\n",
    "\n",
    "def fd_histogram(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def feature_extract(im):\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    feature = bowDiction.compute(gray, sift.detect(gray))\n",
    "    return feature.squeeze() if feature is not None else np.zeros(DICT_SIZE)\n",
    "\n",
    "# Get the absolute image path\n",
    "base = Path(IMDIR).resolve()\n",
    "print(f\"üìÅ Image directory: {base}\")\n",
    "\n",
    "# Initialize SIFT and BOW trainer\n",
    "sift = cv2.SIFT_create()\n",
    "BOW = cv2.BOWKMeansTrainer(DICT_SIZE)\n",
    "\n",
    "print(\"üîç Collecting descriptors...\")\n",
    "for file in base.glob('**/*.*'):\n",
    "    fpath = Path(file).resolve()\n",
    "    image = cv2.imread(str(fpath))\n",
    "    if image is None:\n",
    "        print(f\"‚ùå Could not read image: {fpath}\")\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    kp, dsc = sift.detectAndCompute(gray, None)\n",
    "    if dsc is not None:\n",
    "        BOW.add(dsc)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No descriptors found in: {fpath.name}\")\n",
    "\n",
    "descriptors_list = BOW.getDescriptors()\n",
    "total_desc = sum(d.shape[0] for d in descriptors_list) if descriptors_list else 0\n",
    "print(f\"‚úÖ Total descriptors collected: {total_desc}\")\n",
    "\n",
    "if total_desc == 0:\n",
    "    print(\"‚ùå No descriptors found in any image. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Cluster descriptors and create vocabulary\n",
    "print(\"üìä Clustering descriptors to form visual vocabulary...\")\n",
    "dictionary = BOW.cluster()\n",
    "print(f\"‚úÖ Dictionary shape: {dictionary.shape}\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open(BOVW, \"wb\") as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "\n",
    "# Load vocabulary and set up BOW extractor\n",
    "with open(BOVW, \"rb\") as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "sift2 = cv2.SIFT_create()\n",
    "bowDiction = cv2.BOWImgDescriptorExtractor(sift2, cv2.BFMatcher(cv2.NORM_L2))\n",
    "bowDiction.setVocabulary(dictionary)\n",
    "\n",
    "print(\"üîé Feature Extraction for all images...\")\n",
    "x_data = []\n",
    "x_label = []\n",
    "\n",
    "for file in base.glob('**/*.*'):\n",
    "    fpath = Path(file).resolve()\n",
    "    image = cv2.imread(str(fpath))\n",
    "    if image is None:\n",
    "        print(f\"‚ùå Could not read image: {fpath}\")\n",
    "        continue\n",
    "\n",
    "    humo = fd_hu_moments(image)\n",
    "    harl = fd_haralick(image)\n",
    "    hist = fd_histogram(image)\n",
    "    bovw = feature_extract(image)\n",
    "\n",
    "    features = np.hstack([humo, harl, hist, bovw])\n",
    "    x_data.append(features)\n",
    "    x_label.append(int(fpath.parent.name))\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "x_label = encoder.fit_transform(x_label)\n",
    "\n",
    "# Save features and labels\n",
    "np.save(DATA, np.array(x_data))\n",
    "np.save(LABEL, np.array(x_label))\n",
    "\n",
    "print(\"‚úÖ Feature extraction complete. Data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
